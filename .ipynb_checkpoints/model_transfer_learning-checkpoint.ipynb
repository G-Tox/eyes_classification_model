{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_close='dataset/ClosedFace'\n",
    "dir_train_close='dataset/train/close'\n",
    "dir_test_close='dataset/test/close'\n",
    "dir_open='dataset/OpenFace'\n",
    "dir_train_open='dataset/train/open'\n",
    "dir_test_open='dataset/test/open'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_new = os.listdir(dir_close)\n",
    "num_train=int(len(lista_new)*0.9)\n",
    "lista_train=lista_new[:num_train]\n",
    "lista_test=lista_new[num_train:]\n",
    "\n",
    "for j in lista_train:\n",
    "    copyfile(dir_close+'/'+j,dir_train_close+'/'+j)\n",
    "        \n",
    "for k in lista_test:\n",
    "    copyfile(dir_close+'/'+k,dir_test_close+'/'+k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_new1 = os.listdir(dir_open)\n",
    "num_train1=int(len(lista_new1)*0.9)\n",
    "lista_train1=lista_new1[:num_train1]\n",
    "lista_test1=lista_new1[num_train1:]\n",
    " \n",
    "for l in lista_train1:\n",
    "    copyfile(dir_open+'/'+l,dir_train_open+'/'+l)\n",
    "        \n",
    "for m in lista_test1:\n",
    "    copyfile(dir_open+'/'+m,dir_test_open+'/'+m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(rescale=1./255.,\n",
    "                              rotation_range = 40,\n",
    "                                   width_shift_range = 0.2,\n",
    "                                   height_shift_range = 0.2,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "val_gen = ImageDataGenerator(rescale=1./255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2181 images belonging to 2 classes.\n",
      "Found 242 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir='dataset/train'\n",
    "val_dir='dataset/test'\n",
    "\n",
    "train_generator=train_gen.flow_from_directory(train_dir,target_size=(100,100),class_mode='binary',batch_size=32)\n",
    "val_generator=val_gen.flow_from_directory(val_dir,target_size=(100,100),class_mode='binary',batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(128,(3,3),activation='relu',input_shape=(100,100,3)),\n",
    "    tf.keras.layers.MaxPool2D(2,2),\n",
    "    tf.keras.layers.Conv2D(64,(3,3),activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32,(3,3),activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(2,2),\n",
    "    tf.keras.layers.Conv2D(16,(3,3),activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    #tf.keras.layers.Dense(1024,activation='relu'),\n",
    "    tf.keras.layers.Dense(512,activation='relu'),\n",
    "    #tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001), loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "69/69 [==============================] - 55s 803ms/step - loss: 0.7016 - acc: 0.5397 - val_loss: 0.6625 - val_acc: 0.6529\n",
      "Epoch 2/20\n",
      "69/69 [==============================] - 49s 710ms/step - loss: 0.6811 - acc: 0.5951 - val_loss: 0.6413 - val_acc: 0.6198\n",
      "Epoch 3/20\n",
      "69/69 [==============================] - 48s 703ms/step - loss: 0.6571 - acc: 0.6245 - val_loss: 0.6345 - val_acc: 0.6818\n",
      "Epoch 4/20\n",
      "69/69 [==============================] - 49s 705ms/step - loss: 0.6580 - acc: 0.6181 - val_loss: 0.6926 - val_acc: 0.5207\n",
      "Epoch 5/20\n",
      "69/69 [==============================] - 50s 729ms/step - loss: 0.6365 - acc: 0.6364 - val_loss: 0.5596 - val_acc: 0.6942\n",
      "Epoch 6/20\n",
      "69/69 [==============================] - 51s 742ms/step - loss: 0.6079 - acc: 0.6703 - val_loss: 0.7053 - val_acc: 0.5661\n",
      "Epoch 7/20\n",
      "69/69 [==============================] - 52s 756ms/step - loss: 0.6099 - acc: 0.6529 - val_loss: 0.6335 - val_acc: 0.5992\n",
      "Epoch 8/20\n",
      "69/69 [==============================] - 53s 767ms/step - loss: 0.5835 - acc: 0.6951 - val_loss: 0.5390 - val_acc: 0.7397\n",
      "Epoch 9/20\n",
      "69/69 [==============================] - 52s 758ms/step - loss: 0.5703 - acc: 0.7139 - val_loss: 0.4462 - val_acc: 0.7727\n",
      "Epoch 10/20\n",
      "69/69 [==============================] - 53s 762ms/step - loss: 0.5441 - acc: 0.7341 - val_loss: 0.4579 - val_acc: 0.7769\n",
      "Epoch 11/20\n",
      "69/69 [==============================] - 50s 729ms/step - loss: 0.5266 - acc: 0.7460 - val_loss: 0.5435 - val_acc: 0.7355\n",
      "Epoch 12/20\n",
      "69/69 [==============================] - 49s 714ms/step - loss: 0.5175 - acc: 0.7561 - val_loss: 0.4231 - val_acc: 0.8388\n",
      "Epoch 13/20\n",
      "69/69 [==============================] - 51s 735ms/step - loss: 0.4775 - acc: 0.7735 - val_loss: 0.3633 - val_acc: 0.8636\n",
      "Epoch 14/20\n",
      "69/69 [==============================] - 50s 719ms/step - loss: 0.4572 - acc: 0.7877 - val_loss: 0.3529 - val_acc: 0.8595\n",
      "Epoch 15/20\n",
      "69/69 [==============================] - 48s 701ms/step - loss: 0.4320 - acc: 0.8083 - val_loss: 0.3452 - val_acc: 0.8512\n",
      "Epoch 16/20\n",
      "69/69 [==============================] - 50s 725ms/step - loss: 0.4234 - acc: 0.8221 - val_loss: 0.2900 - val_acc: 0.8719\n",
      "Epoch 17/20\n",
      "69/69 [==============================] - 52s 753ms/step - loss: 0.4061 - acc: 0.8326 - val_loss: 0.2948 - val_acc: 0.8760\n",
      "Epoch 18/20\n",
      "69/69 [==============================] - 49s 712ms/step - loss: 0.3677 - acc: 0.8432 - val_loss: 0.5073 - val_acc: 0.7810\n",
      "Epoch 19/20\n",
      "69/69 [==============================] - 49s 703ms/step - loss: 0.3506 - acc: 0.8496 - val_loss: 0.2913 - val_acc: 0.9008\n",
      "Epoch 20/20\n",
      "69/69 [==============================] - 47s 684ms/step - loss: 0.3084 - acc: 0.8634 - val_loss: 0.3656 - val_acc: 0.8471\n"
     ]
    }
   ],
   "source": [
    "hist=model.fit(train_generator,epochs=20,verbose=1,\n",
    "                              validation_data=val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'box': [193, 29, 372, 480], 'confidence': 0.9996830224990845, 'keypoints': {'left_eye': (292, 230), 'right_eye': (469, 232), 'nose': (375, 326), 'mouth_left': (302, 399), 'mouth_right': (459, 399)}}]\n"
     ]
    }
   ],
   "source": [
    "from mtcnn.mtcnn import MTCNN\n",
    "import cv2\n",
    "img = cv2.imread(\"dataset/img_test/cerrar-ojos-1.jpg\")\n",
    "detector = MTCNN()\n",
    "print(detector.detect_faces(img))\n",
    "recog=detector.detect_faces(img)\n",
    "box=recog[0]['box']\n",
    "x=box[0]\n",
    "y=box[1]\n",
    "width=box[2]\n",
    "height=box[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n",
      " is a cerrado\n"
     ]
    }
   ],
   "source": [
    "crop_img = img[y:y+height, x:x+width]\n",
    "output = cv2.resize(crop_img, (100,100))\n",
    "from tensorflow.keras.preprocessing import image\n",
    "x=image.img_to_array(output)\n",
    "x=np.expand_dims(x, axis=0)\n",
    "images = np.vstack([x])\n",
    "\n",
    "classes = model.predict(images, batch_size=10)\n",
    "print(classes[0])\n",
    "if classes[0]>0:\n",
    "    print(\" is a abierto\") \n",
    "else:\n",
    "    print(\" is a cerrado\")\n",
    "\n",
    "#cv2.imshow(\"cropped\", output)\n",
    "#cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "path='dataset/img_test/ojos-cerrados.jpg'\n",
    "img=image.load_img(path, target_size=(100, 100))\n",
    "  \n",
    "x=image.img_to_array(img)\n",
    "x=np.expand_dims(x, axis=0)\n",
    "images = np.vstack([x])\n",
    "  \n",
    "classes = model.predict(images, batch_size=10)\n",
    "  \n",
    "print(classes[0])\n",
    "  \n",
    "if classes[0]>0:\n",
    "    print(\" is a abierto\")\n",
    "    \n",
    "else:\n",
    "    print(\" is a cerrado\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "local_weights_file = 'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "pre_trained_model = InceptionV3(input_shape = (100, 100, 3), \n",
    "                                include_top = False, \n",
    "                                weights = None)\n",
    "\n",
    "pre_trained_model.load_weights(local_weights_file)\n",
    "\n",
    "for layer in pre_trained_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# pre_trained_model.summary()\n",
    "\n",
    "last_layer = pre_trained_model.get_layer('mixed7')\n",
    "print('last layer output shape: ', last_layer.output_shape)\n",
    "last_output = last_layer.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# Flatten the output layer to 1 dimension\n",
    "x = layers.Flatten()(last_output)\n",
    "# Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "# Add a dropout rate of 0.2\n",
    "x = layers.Dropout(0.2)(x)                  \n",
    "# Add a final sigmoid layer for classification\n",
    "x = layers.Dense  (1, activation='sigmoid')(x)           \n",
    "\n",
    "model = Model( pre_trained_model.input, x) \n",
    "\n",
    "model.compile(optimizer = RMSprop(lr=0.0001), \n",
    "              loss = 'binary_crossentropy', \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_transfer = model.fit(\n",
    "            train_generator,\n",
    "            validation_data = val_generator,\n",
    "            steps_per_epoch = 69,\n",
    "            epochs = 20,\n",
    "            validation_steps = 8,\n",
    "            verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "path='dataset/img_test/ojos-abiertos.jpg'\n",
    "img=image.load_img(path, target_size=(100, 100))\n",
    "  \n",
    "x=image.img_to_array(img)\n",
    "x=np.expand_dims(x, axis=0)\n",
    "images = np.vstack([x])\n",
    "  \n",
    "classes = model.predict(images, batch_size=10)\n",
    "  \n",
    "print(classes[0])\n",
    "  \n",
    "if classes[0]>0:\n",
    "    print(\" is a abierto\")\n",
    "    \n",
    "else:\n",
    "    print(\" is a cerrado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
