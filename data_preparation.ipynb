{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from shutil import copyfile\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_close_p = 'dataset/dataset_mrl/close'\n",
    "dir_open_p = 'dataset/dataset_mrl/open'\n",
    "dir_train_close='dataset/train_o/close'\n",
    "dir_test_close='dataset/test_o/close'\n",
    "dir_train_open='dataset/train_o/open'\n",
    "dir_test_open='dataset/test_o/open'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_close_left='dataset/dataset_eyes/closedLeftEyes'\n",
    "dir_close_right='dataset/dataset_eyes/closedRightEyes'\n",
    "dir_train_close='dataset/train/close'\n",
    "dir_test_close='dataset/test/close'\n",
    "dir_open_left='dataset/dataset_eyes/openLeftEyes'\n",
    "dir_open_right='dataset/dataset_eyes/openRightEyes'\n",
    "dir_train_open='dataset/train/open'\n",
    "dir_test_open='dataset/test/open'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(dir_input,dir_train,dir_test):  \n",
    "    lista_new = os.listdir(dir_input)\n",
    "    num_train=int(len(lista_new)*0.9)\n",
    "    lista_random=random.sample(lista_new,len(lista_new))\n",
    "    lista_train=lista_random[:num_train]\n",
    "    lista_test=lista_random[num_train:]\n",
    "\n",
    "    for j in lista_train:\n",
    "        copyfile(dir_input+'/'+j,dir_train+'/'+j)\n",
    "\n",
    "    for k in lista_test:\n",
    "        copyfile(dir_input+'/'+k,dir_test+'/'+k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset(dir_close_p,dir_train_close,dir_test_close)\n",
    "dataset(dir_open_p,dir_train_open,dir_test_open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset(dir_close_left,dir_train_close,dir_test_close)\n",
    "dataset(dir_close_right,dir_train_close,dir_test_close)\n",
    "dataset(dir_open_left,dir_train_open,dir_test_open)\n",
    "dataset(dir_open_right,dir_train_open,dir_test_open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import requests\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Download data\n",
    "face = \"haarcascade.xml\"\n",
    "\n",
    "eye = \"eyecascade.xml\"\n",
    "\n",
    "image = \"dataset/data_own/IMG_5906.JPG\"\n",
    "\n",
    "\n",
    "#Classifiers\n",
    "face_cascade = cv2.CascadeClassifier(face)\n",
    "eye_cascade = cv2.CascadeClassifier(eye)\n",
    "\n",
    "#Image we will predict\n",
    "img = cv2.imread(image)\n",
    "plt.imshow(img)\n",
    "\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Detect face\n",
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "\n",
    "for (x,y,w,h) in faces:\n",
    "\t#params = (image, position, width and height, color in RGB scale, and thickness)\n",
    "\timg = cv2.rectangle(img, (x,y), (x+w, y+h), (255,0,0), 2)\n",
    "\troi_gray = gray[y:y+h, x:x+w]\n",
    "\troi_color = img[y:y+h, x:x+w]\n",
    "\t#Detect eyes per face\n",
    "\teyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "\tfor (ex,ey,ew,eh) in eyes:\n",
    "\t\tcv2.rectangle(roi_color, (ex,ey), (ex+ew, ey+eh), (0,255,0), 2)\n",
    "\n",
    "\n",
    "#Show image\n",
    "cv2.namedWindow(\"image\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "while 1:\n",
    "\tcv2.imshow(\"image\", img)\n",
    "\tcv2.waitKey(1)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Inspira\\.conda\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from mtcnn.mtcnn import MTCNN\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Inspira\\.conda\\envs\\NLP\\lib\\site-packages\\mtcnn\\mtcnn.py:187: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Inspira\\.conda\\envs\\NLP\\lib\\site-packages\\mtcnn\\mtcnn.py:193: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Inspira\\.conda\\envs\\NLP\\lib\\site-packages\\mtcnn\\network.py:43: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Inspira\\.conda\\envs\\NLP\\lib\\site-packages\\mtcnn\\layer_factory.py:88: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Inspira\\.conda\\envs\\NLP\\lib\\site-packages\\mtcnn\\layer_factory.py:79: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Inspira\\.conda\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Inspira\\.conda\\envs\\NLP\\lib\\site-packages\\mtcnn\\layer_factory.py:171: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Inspira\\.conda\\envs\\NLP\\lib\\site-packages\\mtcnn\\layer_factory.py:221: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From C:\\Users\\Inspira\\.conda\\envs\\NLP\\lib\\site-packages\\mtcnn\\layer_factory.py:196: The name tf.nn.xw_plus_b is deprecated. Please use tf.compat.v1.nn.xw_plus_b instead.\n",
      "\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[{'box': [1114, 582, 1246, 1563], 'confidence': 0.998357355594635, 'keypoints': {'left_eye': (1586, 1171), 'right_eye': (2164, 1124), 'nose': (1977, 1514), 'mouth_left': (1666, 1808), 'mouth_right': (2169, 1781)}}]\n",
      "[{'box': [1165, 481, 1249, 1567], 'confidence': 0.9994521737098694, 'keypoints': {'left_eye': (1614, 1066), 'right_eye': (2193, 1042), 'nose': (1965, 1408), 'mouth_left': (1669, 1701), 'mouth_right': (2168, 1695)}}]\n",
      "[]\n",
      "[]\n",
      "[{'box': [935, 558, 1294, 1597], 'confidence': 0.9970249533653259, 'keypoints': {'left_eye': (1341, 1128), 'right_eye': (1952, 1102), 'nose': (1682, 1478), 'mouth_left': (1395, 1776), 'mouth_right': (1940, 1772)}}]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[{'box': [1372, 1065, 294, 313], 'confidence': 0.8800289034843445, 'keypoints': {'left_eye': (1481, 1162), 'right_eye': (1598, 1164), 'nose': (1537, 1214), 'mouth_left': (1481, 1298), 'mouth_right': (1574, 1299)}}]\n",
      "[{'box': [1047, 556, 1280, 1603], 'confidence': 0.9999114274978638, 'keypoints': {'left_eye': (1483, 1192), 'right_eye': (2064, 1166), 'nose': (1830, 1526), 'mouth_left': (1528, 1791), 'mouth_right': (2053, 1785)}}]\n",
      "[{'box': [1075, 609, 1212, 1575], 'confidence': 0.9999352693557739, 'keypoints': {'left_eye': (1518, 1235), 'right_eye': (2082, 1196), 'nose': (1895, 1550), 'mouth_left': (1580, 1835), 'mouth_right': (2089, 1807)}}, {'box': [2837, 2147, 24, 29], 'confidence': 0.7837182283401489, 'keypoints': {'left_eye': (2845, 2160), 'right_eye': (2855, 2158), 'nose': (2852, 2165), 'mouth_left': (2847, 2171), 'mouth_right': (2856, 2170)}}]\n",
      "[{'box': [963, 452, 1264, 1626], 'confidence': 0.9997304081916809, 'keypoints': {'left_eye': (1427, 1124), 'right_eye': (2010, 1117), 'nose': (1790, 1464), 'mouth_left': (1472, 1742), 'mouth_right': (1993, 1744)}}]\n",
      "[{'box': [936, 479, 1196, 1591], 'confidence': 0.9991704225540161, 'keypoints': {'left_eye': (1368, 1131), 'right_eye': (1944, 1113), 'nose': (1765, 1473), 'mouth_left': (1436, 1747), 'mouth_right': (1951, 1732)}}]\n",
      "[{'box': [898, 441, 1150, 1543], 'confidence': 0.9968889355659485, 'keypoints': {'left_eye': (1216, 1083), 'right_eye': (1766, 1075), 'nose': (1484, 1414), 'mouth_left': (1248, 1663), 'mouth_right': (1738, 1667)}}]\n",
      "[{'box': [1093, 408, 1217, 1597], 'confidence': 0.9990384578704834, 'keypoints': {'left_eye': (1451, 1067), 'right_eye': (2029, 1030), 'nose': (1769, 1384), 'mouth_left': (1512, 1667), 'mouth_right': (2021, 1642)}}, {'box': [1598, 2324, 28, 35], 'confidence': 0.7661330103874207, 'keypoints': {'left_eye': (1611, 2335), 'right_eye': (1624, 2334), 'nose': (1622, 2343), 'mouth_left': (1612, 2351), 'mouth_right': (1623, 2350)}}]\n",
      "[{'box': [1157, 476, 1167, 1471], 'confidence': 0.9985381364822388, 'keypoints': {'left_eye': (1460, 1063), 'right_eye': (1990, 1053), 'nose': (1669, 1368), 'mouth_left': (1483, 1641), 'mouth_right': (1936, 1647)}}]\n",
      "[{'box': [1146, 530, 1137, 1401], 'confidence': 0.9999861717224121, 'keypoints': {'left_eye': (1419, 1037), 'right_eye': (1945, 1054), 'nose': (1615, 1362), 'mouth_left': (1440, 1609), 'mouth_right': (1873, 1634)}}]\n",
      "[{'box': [1242, 526, 1059, 1359], 'confidence': 0.9956793189048767, 'keypoints': {'left_eye': (1444, 1038), 'right_eye': (1926, 1061), 'nose': (1561, 1357), 'mouth_left': (1435, 1601), 'mouth_right': (1834, 1627)}}]\n",
      "[{'box': [1010, 506, 1087, 1339], 'confidence': 0.9998617172241211, 'keypoints': {'left_eye': (1272, 1001), 'right_eye': (1782, 1021), 'nose': (1443, 1305), 'mouth_left': (1288, 1565), 'mouth_right': (1701, 1589)}}]\n",
      "[{'box': [1192, 597, 1069, 1335], 'confidence': 0.9994733929634094, 'keypoints': {'left_eye': (1488, 1113), 'right_eye': (1977, 1112), 'nose': (1695, 1421), 'mouth_left': (1526, 1655), 'mouth_right': (1921, 1653)}}]\n",
      "[{'box': [1322, 597, 1041, 1301], 'confidence': 0.9992300271987915, 'keypoints': {'left_eye': (1630, 1116), 'right_eye': (2108, 1101), 'nose': (1861, 1384), 'mouth_left': (1680, 1640), 'mouth_right': (2066, 1637)}}]\n",
      "[{'box': [1358, 628, 1019, 1296], 'confidence': 0.9992235898971558, 'keypoints': {'left_eye': (1646, 1136), 'right_eye': (2117, 1131), 'nose': (1851, 1413), 'mouth_left': (1666, 1644), 'mouth_right': (2072, 1655)}}]\n",
      "[{'box': [1052, 618, 1065, 1337], 'confidence': 0.9986061453819275, 'keypoints': {'left_eye': (1326, 1152), 'right_eye': (1812, 1144), 'nose': (1518, 1438), 'mouth_left': (1352, 1686), 'mouth_right': (1763, 1689)}}]\n",
      "[{'box': [1025, 577, 1005, 1314], 'confidence': 0.99275803565979, 'keypoints': {'left_eye': (1234, 1087), 'right_eye': (1675, 1087), 'nose': (1356, 1371), 'mouth_left': (1246, 1614), 'mouth_right': (1616, 1616)}}, {'box': [526, 1961, 77, 88], 'confidence': 0.7169904708862305, 'keypoints': {'left_eye': (553, 1993), 'right_eye': (587, 1993), 'nose': (571, 2010), 'mouth_left': (556, 2031), 'mouth_right': (581, 2032)}}]\n"
     ]
    }
   ],
   "source": [
    "lista_foto_open = os.listdir('dataset/data_own/open_l')\n",
    "\n",
    "for name in lista_foto_open:\n",
    "    img = cv2.imread('dataset/data_own/open_l/'+name)\n",
    "    detector = MTCNN()\n",
    "    recog=detector.detect_faces(img)\n",
    "    print(recog)\n",
    "    \n",
    "    if recog:        \n",
    "        box=recog[0]['box']\n",
    "        ojo_iz = recog[0]['keypoints']['left_eye']\n",
    "        ojo_de = recog[0]['keypoints']['right_eye']\n",
    "        x=box[0]\n",
    "        y=box[1]\n",
    "        w=box[2]\n",
    "        h=box[3]\n",
    "\n",
    "        ojoiz_x=ojo_iz[0]\n",
    "        ojoiz_y=ojo_iz[1]\n",
    "\n",
    "        ojosd_x=ojo_de[0]\n",
    "        ojosd_y=ojo_de[1]\n",
    "\n",
    "        ojoiz_new_x = ojoiz_x - 230\n",
    "        ojoiz_new_y = ojoiz_y - 150\n",
    "        ojo_w = 450\n",
    "        ojo_h = 300\n",
    "\n",
    "        ojode_new_x = ojosd_x - 200\n",
    "        ojode_new_y = ojosd_y - 150\n",
    "\n",
    "        #img = cv2.rectangle(img, (ojoiz_new_x,ojoiz_new_y), (ojoiz_new_x+ojo_w, ojoiz_new_y+ojo_h), (100,100,100), 2)\n",
    "        #img = cv2.rectangle(img, (ojode_new_x,ojode_new_y), (ojode_new_x+ojo_w, ojode_new_y+ojo_h), (25,100,45), 2)\n",
    "        #img = cv2.rectangle(img, (x,y), (x+w, y+h), (255,0,0), 2)\n",
    "        #img = cv2.circle(img,(ojoiz_x,ojoiz_y),10,(255,0,255),3)\n",
    "        #img = cv2.circle(img,(ojosd_x,ojosd_y),20,(255,0,255),3)\n",
    "\n",
    "        name_eye = name[:-4]\n",
    "\n",
    "        crop_ojo_der = img[ojode_new_y:ojode_new_y+ojo_h, ojode_new_x:ojode_new_x+ojo_w]\n",
    "        cv2.imwrite(\"dataset/data_own/data_eye_open/right_\"+name_eye+\".jpg\", crop_ojo_der)\n",
    "\n",
    "        crop_ojo_iz = img[ojoiz_new_y:ojoiz_new_y+ojo_h, ojoiz_new_x:ojoiz_new_x+ojo_w]\n",
    "        cv2.imwrite(\"dataset/data_own/data_eye_open/left_\"+name_eye+\".jpg\", crop_ojo_iz)\n",
    "\n",
    "        #cv2.imwrite(\"dataset/data_own/IMG_5906_mo.JPG\", img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'box': [938, 540, 1323, 1643], 'confidence': 0.9974242448806763, 'keypoints': {'left_eye': (1309, 1137), 'right_eye': (1924, 1087), 'nose': (1629, 1486), 'mouth_left': (1377, 1789), 'mouth_right': (1934, 1769)}}]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[{'box': [1129, 403, 1217, 1606], 'confidence': 0.9986528158187866, 'keypoints': {'left_eye': (1506, 1062), 'right_eye': (2088, 1063), 'nose': (1810, 1395), 'mouth_left': (1552, 1674), 'mouth_right': (2056, 1675)}}]\n",
      "[{'box': [1217, 469, 1221, 1578], 'confidence': 0.9996863603591919, 'keypoints': {'left_eye': (1640, 1114), 'right_eye': (2211, 1136), 'nose': (1947, 1451), 'mouth_left': (1643, 1703), 'mouth_right': (2169, 1730)}}]\n",
      "[{'box': [1156, 474, 1219, 1566], 'confidence': 0.9998941421508789, 'keypoints': {'left_eye': (1598, 1115), 'right_eye': (2157, 1110), 'nose': (1942, 1414), 'mouth_left': (1621, 1683), 'mouth_right': (2141, 1676)}}]\n",
      "[{'box': [1035, 437, 1219, 1576], 'confidence': 0.9999237060546875, 'keypoints': {'left_eye': (1525, 1081), 'right_eye': (2073, 1080), 'nose': (1899, 1378), 'mouth_left': (1543, 1655), 'mouth_right': (2045, 1653)}}]\n",
      "[{'box': [1025, 433, 1213, 1558], 'confidence': 0.9999032020568848, 'keypoints': {'left_eye': (1501, 1067), 'right_eye': (2056, 1061), 'nose': (1875, 1353), 'mouth_left': (1523, 1631), 'mouth_right': (2044, 1627)}}]\n",
      "[{'box': [1054, 381, 1187, 1561], 'confidence': 0.9992768168449402, 'keypoints': {'left_eye': (1402, 1018), 'right_eye': (1977, 1039), 'nose': (1684, 1360), 'mouth_left': (1408, 1586), 'mouth_right': (1942, 1618)}}]\n",
      "[{'box': [1115, 488, 1208, 1538], 'confidence': 0.9989394545555115, 'keypoints': {'left_eye': (1507, 1117), 'right_eye': (2081, 1115), 'nose': (1817, 1441), 'mouth_left': (1526, 1678), 'mouth_right': (2056, 1687)}}]\n",
      "[{'box': [1055, 418, 1264, 1583], 'confidence': 0.9993670582771301, 'keypoints': {'left_eye': (1468, 1070), 'right_eye': (2059, 1065), 'nose': (1788, 1401), 'mouth_left': (1493, 1654), 'mouth_right': (2032, 1655)}}]\n",
      "[{'box': [960, 393, 1275, 1612], 'confidence': 0.9997714161872864, 'keypoints': {'left_eye': (1404, 1083), 'right_eye': (1991, 1086), 'nose': (1727, 1417), 'mouth_left': (1426, 1668), 'mouth_right': (1962, 1677)}}]\n",
      "[{'box': [1325, 259, 1188, 1559], 'confidence': 0.9995909333229065, 'keypoints': {'left_eye': (1675, 896), 'right_eye': (2237, 901), 'nose': (1954, 1233), 'mouth_left': (1681, 1455), 'mouth_right': (2204, 1466)}}]\n",
      "[{'box': [1257, 496, 1170, 1509], 'confidence': 0.9976420998573303, 'keypoints': {'left_eye': (1599, 1115), 'right_eye': (2165, 1135), 'nose': (1865, 1455), 'mouth_left': (1607, 1678), 'mouth_right': (2119, 1706)}}]\n",
      "[{'box': [1029, 606, 1036, 1256], 'confidence': 0.9999754428863525, 'keypoints': {'left_eye': (1283, 1079), 'right_eye': (1758, 1099), 'nose': (1456, 1358), 'mouth_left': (1297, 1598), 'mouth_right': (1700, 1622)}}]\n",
      "[{'box': [1070, 613, 977, 1272], 'confidence': 0.9871978163719177, 'keypoints': {'left_eye': (1254, 1101), 'right_eye': (1698, 1118), 'nose': (1369, 1375), 'mouth_left': (1257, 1613), 'mouth_right': (1633, 1629)}}]\n",
      "[{'box': [957, 636, 1007, 1297], 'confidence': 0.9919344186782837, 'keypoints': {'left_eye': (1146, 1131), 'right_eye': (1590, 1146), 'nose': (1250, 1416), 'mouth_left': (1130, 1648), 'mouth_right': (1519, 1666)}}]\n",
      "[{'box': [1236, 643, 1053, 1309], 'confidence': 0.9996650218963623, 'keypoints': {'left_eye': (1520, 1163), 'right_eye': (1989, 1146), 'nose': (1716, 1432), 'mouth_left': (1557, 1674), 'mouth_right': (1954, 1670)}}]\n",
      "[{'box': [1165, 662, 1050, 1313], 'confidence': 0.9999802112579346, 'keypoints': {'left_eye': (1443, 1187), 'right_eye': (1931, 1190), 'nose': (1649, 1474), 'mouth_left': (1471, 1710), 'mouth_right': (1882, 1718)}}]\n",
      "[{'box': [1196, 653, 1026, 1312], 'confidence': 0.9997379183769226, 'keypoints': {'left_eye': (1475, 1190), 'right_eye': (1948, 1188), 'nose': (1682, 1462), 'mouth_left': (1501, 1694), 'mouth_right': (1914, 1698)}}]\n",
      "[{'box': [1216, 629, 1050, 1322], 'confidence': 0.999298095703125, 'keypoints': {'left_eye': (1494, 1143), 'right_eye': (1981, 1128), 'nose': (1714, 1419), 'mouth_left': (1545, 1663), 'mouth_right': (1947, 1651)}}]\n",
      "[{'box': [1195, 643, 1048, 1313], 'confidence': 0.9998689889907837, 'keypoints': {'left_eye': (1472, 1158), 'right_eye': (1962, 1156), 'nose': (1681, 1450), 'mouth_left': (1507, 1681), 'mouth_right': (1928, 1684)}}]\n",
      "[{'box': [1250, 670, 1042, 1282], 'confidence': 0.9997836947441101, 'keypoints': {'left_eye': (1522, 1178), 'right_eye': (1985, 1165), 'nose': (1707, 1453), 'mouth_left': (1545, 1671), 'mouth_right': (1958, 1665)}}]\n",
      "[{'box': [1308, 683, 1018, 1275], 'confidence': 0.9999054670333862, 'keypoints': {'left_eye': (1542, 1203), 'right_eye': (2011, 1199), 'nose': (1712, 1507), 'mouth_left': (1569, 1704), 'mouth_right': (1976, 1710)}}]\n"
     ]
    }
   ],
   "source": [
    "lista_foto_close = os.listdir('dataset/data_own/close_l')\n",
    "\n",
    "for name in lista_foto_close:\n",
    "    img = cv2.imread('dataset/data_own/close_l/'+name)\n",
    "    detector = MTCNN()\n",
    "    recog=detector.detect_faces(img)\n",
    "    print(recog)\n",
    "    \n",
    "    if recog:\n",
    "        box=recog[0]['box']\n",
    "        ojo_iz = recog[0]['keypoints']['left_eye']\n",
    "        ojo_de = recog[0]['keypoints']['right_eye']\n",
    "        x=box[0]\n",
    "        y=box[1]\n",
    "        w=box[2]\n",
    "        h=box[3]\n",
    "\n",
    "        ojoiz_x=ojo_iz[0]\n",
    "        ojoiz_y=ojo_iz[1]\n",
    "\n",
    "        ojosd_x=ojo_de[0]\n",
    "        ojosd_y=ojo_de[1]\n",
    "\n",
    "        ojoiz_new_x = ojoiz_x - 230\n",
    "        ojoiz_new_y = ojoiz_y - 150\n",
    "        ojo_w = 450\n",
    "        ojo_h = 300\n",
    "\n",
    "        ojode_new_x = ojosd_x - 200\n",
    "        ojode_new_y = ojosd_y - 150\n",
    "\n",
    "        #img = cv2.rectangle(img, (ojoiz_new_x,ojoiz_new_y), (ojoiz_new_x+ojo_w, ojoiz_new_y+ojo_h), (100,100,100), 2)\n",
    "        #img = cv2.rectangle(img, (ojode_new_x,ojode_new_y), (ojode_new_x+ojo_w, ojode_new_y+ojo_h), (25,100,45), 2)\n",
    "        #img = cv2.rectangle(img, (x,y), (x+w, y+h), (255,0,0), 2)\n",
    "        #img = cv2.circle(img,(ojoiz_x,ojoiz_y),10,(255,0,255),3)\n",
    "        #img = cv2.circle(img,(ojosd_x,ojosd_y),20,(255,0,255),3)\n",
    "\n",
    "        name_eye = name[:-4]\n",
    "\n",
    "        crop_ojo_der = img[ojode_new_y:ojode_new_y+ojo_h, ojode_new_x:ojode_new_x+ojo_w]\n",
    "        cv2.imwrite(\"dataset/data_own/data_eye_close/right\"+name_eye+\".jpg\", crop_ojo_der)\n",
    "\n",
    "        crop_ojo_iz = img[ojoiz_new_y:ojoiz_new_y+ojo_h, ojoiz_new_x:ojoiz_new_x+ojo_w]\n",
    "        cv2.imwrite(\"dataset/data_own/data_eye_close/left\"+name_eye+\".jpg\", crop_ojo_iz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_close_p = 'dataset/data_own/data_eye_close'\n",
    "dir_train_close = 'dataset/train_own/close'\n",
    "dir_test_close = 'dataset/valid_own/close'\n",
    "dir_open_p = 'dataset/data_own/data_eye_open'\n",
    "dir_train_open = 'dataset/train_own/open'\n",
    "dir_test_open = 'dataset/valid_own/open'\n",
    "\n",
    "dataset(dir_close_p,dir_train_close,dir_test_close)\n",
    "dataset(dir_open_p,dir_train_open,dir_test_open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'box': [281, 319, 158, 190], 'confidence': 0.9992753863334656, 'keypoints': {'left_eye': (323, 388), 'right_eye': (398, 397), 'nose': (353, 441), 'mouth_left': (320, 468), 'mouth_right': (381, 477)}}]\n",
      "[{'box': [265, 176, 164, 220], 'confidence': 0.9996693134307861, 'keypoints': {'left_eye': (311, 257), 'right_eye': (388, 253), 'nose': (349, 304), 'mouth_left': (320, 350), 'mouth_right': (383, 347)}}]\n",
      "[{'box': [264, 182, 168, 218], 'confidence': 0.9983515739440918, 'keypoints': {'left_eye': (308, 258), 'right_eye': (388, 252), 'nose': (348, 302), 'mouth_left': (320, 350), 'mouth_right': (384, 347)}}]\n",
      "[{'box': [276, 174, 161, 212], 'confidence': 0.9992819428443909, 'keypoints': {'left_eye': (327, 248), 'right_eye': (404, 248), 'nose': (367, 290), 'mouth_left': (333, 338), 'mouth_right': (395, 338)}}]\n",
      "[{'box': [265, 181, 167, 218], 'confidence': 0.9996483325958252, 'keypoints': {'left_eye': (311, 257), 'right_eye': (390, 253), 'nose': (351, 304), 'mouth_left': (319, 350), 'mouth_right': (386, 349)}}]\n",
      "[{'box': [272, 188, 171, 219], 'confidence': 0.999599039554596, 'keypoints': {'left_eye': (312, 266), 'right_eye': (394, 261), 'nose': (352, 322), 'mouth_left': (320, 360), 'mouth_right': (391, 355)}}]\n",
      "[{'box': [279, 182, 173, 221], 'confidence': 0.9992613196372986, 'keypoints': {'left_eye': (329, 266), 'right_eye': (409, 262), 'nose': (371, 316), 'mouth_left': (341, 360), 'mouth_right': (404, 353)}}]\n",
      "[{'box': [269, 192, 179, 222], 'confidence': 0.9969735145568848, 'keypoints': {'left_eye': (323, 273), 'right_eye': (410, 265), 'nose': (372, 324), 'mouth_left': (338, 367), 'mouth_right': (407, 359)}}]\n",
      "[{'box': [273, 190, 175, 219], 'confidence': 0.9995527863502502, 'keypoints': {'left_eye': (329, 270), 'right_eye': (412, 262), 'nose': (376, 317), 'mouth_left': (342, 364), 'mouth_right': (408, 357)}}]\n",
      "[{'box': [300, 194, 173, 223], 'confidence': 0.9987755417823792, 'keypoints': {'left_eye': (359, 271), 'right_eye': (442, 268), 'nose': (407, 314), 'mouth_left': (370, 367), 'mouth_right': (437, 364)}}]\n",
      "[{'box': [265, 198, 182, 223], 'confidence': 0.9990599751472473, 'keypoints': {'left_eye': (319, 283), 'right_eye': (403, 267), 'nose': (371, 328), 'mouth_left': (342, 376), 'mouth_right': (409, 362)}}]\n",
      "[{'box': [288, 204, 179, 224], 'confidence': 0.9989456534385681, 'keypoints': {'left_eye': (345, 283), 'right_eye': (433, 278), 'nose': (396, 336), 'mouth_left': (358, 383), 'mouth_right': (429, 379)}}]\n",
      "[{'box': [275, 191, 176, 225], 'confidence': 0.9989522695541382, 'keypoints': {'left_eye': (324, 279), 'right_eye': (406, 277), 'nose': (363, 334), 'mouth_left': (333, 373), 'mouth_right': (401, 372)}}]\n",
      "[{'box': [282, 199, 172, 220], 'confidence': 0.9993388056755066, 'keypoints': {'left_eye': (332, 282), 'right_eye': (413, 281), 'nose': (372, 332), 'mouth_left': (338, 370), 'mouth_right': (408, 371)}}]\n"
     ]
    }
   ],
   "source": [
    "from mtcnn.mtcnn import MTCNN\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "model = load_model('models/model.h5')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while (True):\n",
    "    ret, img = cap.read()\n",
    "    #frame_id += 1\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    detector = MTCNN()\n",
    "    recog=detector.detect_faces(img)\n",
    "    print(recog)\n",
    "\n",
    "    if recog:\n",
    "        box=recog[0]['box']\n",
    "        x=box[0]\n",
    "        y=box[1]\n",
    "        w=box[2]\n",
    "        h=box[3]\n",
    "\n",
    "        crop_face = img[y-50:(y)+(h+50),(x-50):(x)+(w+50)]\n",
    "        crop_face = cv2.resize(crop_face, (830,900))\n",
    "        #cv2.imwrite(\"dataset/img_test/resize.jpg\", crop_face)\n",
    "        img = cv2.rectangle(img, (x,y), (x+w, y+h), (255,0,0), 10)\n",
    "\n",
    "        detector8 = MTCNN()\n",
    "        recog_eyes = detector8.detect_faces(crop_face)\n",
    "\n",
    "        if recog_eyes:\n",
    "\n",
    "            ojo_iz = recog_eyes[0]['keypoints']['left_eye']\n",
    "            ojo_de = recog_eyes[0]['keypoints']['right_eye']\n",
    "\n",
    "            ojoiz_x=ojo_iz[0]\n",
    "            ojoiz_y=ojo_iz[1]\n",
    "\n",
    "            ojosd_x=ojo_de[0]\n",
    "            ojosd_y=ojo_de[1]\n",
    "\n",
    "            ojoiz_new_x = ojoiz_x - 120\n",
    "            ojoiz_new_y = ojoiz_y - 90\n",
    "            ojo_w = 200\n",
    "            ojo_h = 150\n",
    "\n",
    "            ojode_new_x = ojosd_x - 110\n",
    "            ojode_new_y = ojosd_y - 90\n",
    "\n",
    "            #ojoiz_new_x = ojoiz_x - 230\n",
    "            #ojoiz_new_y = ojoiz_y - 150\n",
    "            #ojo_w = 450\n",
    "            #ojo_h = 300\n",
    "\n",
    "            #ojode_new_x = ojosd_x - 200\n",
    "            #ojode_new_y = ojosd_y - 150\n",
    "\n",
    "\n",
    "            #img = cv2.rectangle(img, (ojoiz_new_x,ojoiz_new_y), (ojoiz_new_x+ojo_w, ojoiz_new_y+ojo_h), (100,100,100), 2)\n",
    "            #img = cv2.rectangle(img, (ojode_new_x,ojode_new_y), (ojode_new_x+ojo_w, ojode_new_y+ojo_h), (25,100,45), 2)\n",
    "            #img = cv2.circle(img,(ojoiz_x,ojoiz_y),10,(255,0,255),3)\n",
    "            #img = cv2.circle(img,(ojosd_x,ojosd_y),20,(255,0,255),3)\n",
    "\n",
    "            #cv2.imwrite(\"dataset/img_test/cerrar-ojos-12.jpg\", img)\n",
    "\n",
    "            crop_ojo_der = crop_face[ojode_new_y:ojode_new_y+ojo_h, ojode_new_x:ojode_new_x+ojo_w]\n",
    "            output = cv2.resize(crop_ojo_der, (200,200))\n",
    "            #cv2.imwrite(\"dataset/img_test/resize_ojo.jpg\", crop_ojo_der)\n",
    "            x=image.img_to_array(output)\n",
    "            x=np.expand_dims(x, axis=0)\n",
    "            images = np.vstack([x])\n",
    "            classes = None\n",
    "            classes = model.predict(images, batch_size=10)\n",
    "\n",
    "            if classes >=0.5:\n",
    "                texto = 'Ojo Izquierdo abierto'\n",
    "                cv2.putText(img, texto, (10,10), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 0, 0), 3)\n",
    "                cv2.putText(img, texto, (10,10), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 255, 0), 2)\n",
    "\n",
    "            elif classes < 0.5:\n",
    "                texto = 'Ojo Izquierdo cerrado'\n",
    "                cv2.putText(img, texto, (10, 10), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 0, 0), 3)\n",
    "                cv2.putText(img, texto, (10, 10), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 255, 0), 2)\n",
    "\n",
    "            crop_ojo_iz = crop_face[ojoiz_new_y:ojoiz_new_y+ojo_h, ojoiz_new_x:ojoiz_new_x+ojo_w]\n",
    "            output_1 = cv2.resize(crop_ojo_iz, (200,200)) \n",
    "            #cv2.imwrite(\"dataset/img_test/resize_1.jpg\", crop_ojo_iz)\n",
    "            x_1=image.img_to_array(output_1)\n",
    "            x_1=np.expand_dims(x_1, axis=0)\n",
    "            images_1 = np.vstack([x_1])\n",
    "            classes_1 = None\n",
    "            classes_1 = model.predict(images_1, batch_size=10)\n",
    "\n",
    "            if classes_1 >=0.5:\n",
    "                texto = 'Ojo derecho abierto'\n",
    "                cv2.putText(img, texto, (30,30), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 0, 0), 3)\n",
    "                cv2.putText(img, texto, (30,30), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 255, 0), 2)\n",
    "\n",
    "            elif classes_1 < 0.5:\n",
    "                texto = 'Ojo derecho cerrado'\n",
    "                cv2.putText(img, texto, (30,30), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 0, 0), 3)\n",
    "                cv2.putText(img, texto, (30,30), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 255, 0), 2)\n",
    "\n",
    "        else:\n",
    "            print(' no se reconocio los ojos')\n",
    "        \n",
    "    cv2.imshow(\"Image\", img)\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\mtcnn\\mtcnn.py:187: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\mtcnn\\mtcnn.py:193: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\mtcnn\\network.py:43: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\mtcnn\\layer_factory.py:88: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\mtcnn\\layer_factory.py:79: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\mtcnn\\layer_factory.py:171: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\mtcnn\\layer_factory.py:221: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\mtcnn\\layer_factory.py:196: The name tf.nn.xw_plus_b is deprecated. Please use tf.compat.v1.nn.xw_plus_b instead.\n",
      "\n",
      "[]\n",
      "[{'box': [271, 210, 163, 211], 'confidence': 0.999963641166687, 'keypoints': {'left_eye': (322, 286), 'right_eye': (400, 286), 'nose': (366, 328), 'mouth_left': (331, 378), 'mouth_right': (393, 376)}}]\n",
      "[{'box': [279, 89, 176, 224], 'confidence': 0.9999502897262573, 'keypoints': {'left_eye': (334, 175), 'right_eye': (418, 175), 'nose': (380, 225), 'mouth_left': (343, 271), 'mouth_right': (410, 271)}}]\n",
      "[{'box': [279, 147, 167, 223], 'confidence': 0.9993670582771301, 'keypoints': {'left_eye': (333, 231), 'right_eye': (412, 231), 'nose': (372, 271), 'mouth_left': (336, 321), 'mouth_right': (403, 321)}}]\n",
      "[{'box': [260, 121, 170, 223], 'confidence': 0.9987215399742126, 'keypoints': {'left_eye': (318, 207), 'right_eye': (397, 207), 'nose': (361, 249), 'mouth_left': (323, 297), 'mouth_right': (390, 297)}}]\n",
      "[{'box': [260, 120, 179, 241], 'confidence': 0.9933046698570251, 'keypoints': {'left_eye': (319, 209), 'right_eye': (402, 209), 'nose': (360, 263), 'mouth_left': (321, 308), 'mouth_right': (393, 308)}}]\n",
      "[{'box': [254, 118, 179, 235], 'confidence': 0.9972037076950073, 'keypoints': {'left_eye': (313, 208), 'right_eye': (395, 209), 'nose': (357, 263), 'mouth_left': (319, 308), 'mouth_right': (387, 309)}}]\n",
      "[{'box': [259, 121, 177, 239], 'confidence': 0.9960565567016602, 'keypoints': {'left_eye': (321, 210), 'right_eye': (402, 211), 'nose': (366, 260), 'mouth_left': (325, 308), 'mouth_right': (392, 309)}}]\n",
      "[{'box': [257, 120, 180, 237], 'confidence': 0.9989043474197388, 'keypoints': {'left_eye': (315, 208), 'right_eye': (400, 207), 'nose': (361, 259), 'mouth_left': (322, 305), 'mouth_right': (392, 304)}}]\n",
      "[{'box': [250, 135, 190, 257], 'confidence': 0.9935408234596252, 'keypoints': {'left_eye': (322, 235), 'right_eye': (408, 231), 'nose': (384, 300), 'mouth_left': (331, 345), 'mouth_right': (404, 342)}}]\n"
     ]
    }
   ],
   "source": [
    "from mtcnn.mtcnn import MTCNN\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "model = load_model('models/model_1.h5')\n",
    "\n",
    "#Creamos un objeto que iniciará la captura de video en la primera entrada disponible (la 0)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "#Iniciamos un bucle continua\n",
    "while(True):\n",
    "    #Dos objetos irán grabando las imágenes del objeto cap\n",
    "    ret, img = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    detector = MTCNN()\n",
    "    recog=detector.detect_faces(img)\n",
    "    print(recog)\n",
    "\n",
    "    if recog:\n",
    "        box=recog[0]['box']\n",
    "        x=box[0]\n",
    "        y=box[1]\n",
    "        w=box[2]\n",
    "        h=box[3]\n",
    "\n",
    "        crop_face = img[y-50:(y)+(h+50),(x-50):(x)+(w+50)]\n",
    "        crop_face = cv2.resize(crop_face, (830,900))\n",
    "        #cv2.imwrite(\"dataset/img_test/resize.jpg\", crop_face)\n",
    "        img = cv2.rectangle(img, (x,y), (x+w, y+h), (255,0,0), 2)\n",
    "        \n",
    "        detector8 = MTCNN()\n",
    "        recog_eyes = detector8.detect_faces(crop_face)\n",
    "\n",
    "        if recog_eyes:\n",
    "\n",
    "            ojo_iz = recog_eyes[0]['keypoints']['left_eye']\n",
    "            ojo_de = recog_eyes[0]['keypoints']['right_eye']\n",
    "\n",
    "            ojoiz_x=ojo_iz[0]\n",
    "            ojoiz_y=ojo_iz[1]\n",
    "\n",
    "            ojosd_x=ojo_de[0]\n",
    "            ojosd_y=ojo_de[1]\n",
    "\n",
    "            ojoiz_new_x = ojoiz_x - 120\n",
    "            ojoiz_new_y = ojoiz_y - 90\n",
    "            ojo_w = 200\n",
    "            ojo_h = 150\n",
    "\n",
    "            ojode_new_x = ojosd_x - 110\n",
    "            ojode_new_y = ojosd_y - 90\n",
    "            \n",
    "    cv2.imshow('Hola Mundo',img)\n",
    "\n",
    "    #el procedimiento waitKey comprueba si se ha pulsado la tecla 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Si se ha roto el bucle, procedemos a destruir la ventana y finalizar el programa\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "crop_ojo_der = img[ojode_new_y:ojode_new_y+ojo_h, ojode_new_x:ojode_new_x+ojo_w]\n",
    "cv2.imwrite(\"dataset/img_test/resize.jpg\", crop_ojo_der)\n",
    "output = cv2.resize(crop_ojo_der, (200,200))\n",
    "x=image.img_to_array(output)\n",
    "x=np.expand_dims(x, axis=0)\n",
    "images = np.vstack([x])\n",
    "classes = None\n",
    "classes = model.predict(images, batch_size=10)\n",
    "print(classes[0])\n",
    "    \n",
    "crop_ojo_iz = img[ojoiz_new_y:ojoiz_new_y+ojo_h, ojoiz_new_x:ojoiz_new_x+ojo_w]\n",
    "output_1 = cv2.resize(crop_ojo_iz, (200,200)) \n",
    "cv2.imwrite(\"dataset/img_test/resize_1.jpg\", output_1)\n",
    "x_1=image.img_to_array(output_1)\n",
    "x_1=np.expand_dims(x_1, axis=0)\n",
    "images_1 = np.vstack([x_1])\n",
    "classes_1 = None\n",
    "classes_1 = model.predict(images_1, batch_size=10)\n",
    "print(classes_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Inspira\\.conda\\envs\\NLP-GPU\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14898910522840973515\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3203543859\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 13927022658980315416\n",
      "physical_device_desc: \"device: 0, name: GeForce MX130, pci bus id: 0000:01:00.0, compute capability: 5.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'box': [434, 52, 266, 358], 'confidence': 0.999790370464325, 'keypoints': {'left_eye': (511, 197), 'right_eye': (635, 197), 'nose': (562, 265), 'mouth_left': (514, 329), 'mouth_right': (620, 329)}}]\n"
     ]
    }
   ],
   "source": [
    "from mtcnn.mtcnn import MTCNN\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "model = load_model('models/model_1.h5')\n",
    "\n",
    "img = cv2.imread(\"dataset/img_test/ojos-cerrados.jpg\")\n",
    "detector = MTCNN()\n",
    "recog=detector.detect_faces(img)\n",
    "print(recog)\n",
    "\n",
    "if recog:\n",
    "    box=recog[0]['box']\n",
    "    x=box[0]\n",
    "    y=box[1]\n",
    "    w=box[2]\n",
    "    h=box[3]\n",
    "\n",
    "    crop_face = img[y-50:(y)+(h+50),(x-50):(x)+(w+50)]\n",
    "    crop_face = cv2.resize(crop_face, (830,900))\n",
    "        #cv2.imwrite(\"dataset/img_test/resize.jpg\", crop_face)\n",
    "    img = cv2.rectangle(img, (x,y), (x+w, y+h), (255,0,0), 5)\n",
    "\n",
    "    detector8 = MTCNN()\n",
    "    recog_eyes = detector8.detect_faces(crop_face)\n",
    "\n",
    "    if recog_eyes:\n",
    "\n",
    "        ojo_iz = recog_eyes[0]['keypoints']['left_eye']\n",
    "        ojo_de = recog_eyes[0]['keypoints']['right_eye']\n",
    "\n",
    "        ojoiz_x=ojo_iz[0]\n",
    "        ojoiz_y=ojo_iz[1]\n",
    "\n",
    "        ojosd_x=ojo_de[0]\n",
    "        ojosd_y=ojo_de[1]\n",
    "\n",
    "        ojoiz_new_x = ojoiz_x - 120\n",
    "        ojoiz_new_y = ojoiz_y - 90\n",
    "        ojo_w = 200\n",
    "        ojo_h = 150\n",
    "\n",
    "        ojode_new_x = ojosd_x - 110\n",
    "        ojode_new_y = ojosd_y - 90\n",
    "\n",
    "            #ojoiz_new_x = ojoiz_x - 230\n",
    "            #ojoiz_new_y = ojoiz_y - 150\n",
    "            #ojo_w = 450\n",
    "            #ojo_h = 300\n",
    "\n",
    "            #ojode_new_x = ojosd_x - 200\n",
    "            #ojode_new_y = ojosd_y - 150\n",
    "\n",
    "\n",
    "            #img = cv2.rectangle(img, (ojoiz_new_x,ojoiz_new_y), (ojoiz_new_x+ojo_w, ojoiz_new_y+ojo_h), (100,100,100), 2)\n",
    "            #img = cv2.rectangle(img, (ojode_new_x,ojode_new_y), (ojode_new_x+ojo_w, ojode_new_y+ojo_h), (25,100,45), 2)\n",
    "            #img = cv2.circle(img,(ojoiz_x,ojoiz_y),10,(255,0,255),3)\n",
    "            #img = cv2.circle(img,(ojosd_x,ojosd_y),20,(255,0,255),3)\n",
    "\n",
    "            #cv2.imwrite(\"dataset/img_test/cerrar-ojos-12.jpg\", img)\n",
    "\n",
    "        crop_ojo_der = crop_face[ojode_new_y:ojode_new_y+ojo_h, ojode_new_x:ojode_new_x+ojo_w]\n",
    "        output = cv2.resize(crop_ojo_der, (200,200))\n",
    "            #cv2.imwrite(\"dataset/img_test/resize_ojo.jpg\", crop_ojo_der)\n",
    "        x=image.img_to_array(output)\n",
    "        x=np.expand_dims(x, axis=0)\n",
    "        images = np.vstack([x])\n",
    "        classes = None\n",
    "        classes = model.predict(images, batch_size=10)\n",
    "\n",
    "        if classes >=0.5:\n",
    "            texto = 'Ojo Izquierdo abierto'\n",
    "            cv2.putText(img, texto, (10,10), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 0, 0), 3)\n",
    "            cv2.putText(img, texto, (10,10), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 255, 0), 2)\n",
    "\n",
    "        elif classes < 0.5:\n",
    "            texto = 'Ojo Izquierdo cerrado'\n",
    "            cv2.putText(img, texto, (10,10), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 0, 0), 3)\n",
    "            cv2.putText(img, texto, (10,10), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 255, 0), 2)\n",
    "\n",
    "        crop_ojo_iz = crop_face[ojoiz_new_y:ojoiz_new_y+ojo_h, ojoiz_new_x:ojoiz_new_x+ojo_w]\n",
    "        output_1 = cv2.resize(crop_ojo_iz, (200,200)) \n",
    "            #cv2.imwrite(\"dataset/img_test/resize_1.jpg\", crop_ojo_iz)\n",
    "        x_1=image.img_to_array(output_1)\n",
    "        x_1=np.expand_dims(x_1, axis=0)\n",
    "        images_1 = np.vstack([x_1])\n",
    "        classes_1 = None\n",
    "        classes_1 = model.predict(images_1, batch_size=10)\n",
    "\n",
    "        if classes_1 >=0.5:\n",
    "            texto = 'Ojo derecho abierto'\n",
    "            cv2.putText(img, texto, (30,30), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 0, 0), 3)\n",
    "            cv2.putText(img, texto, (30,30), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 255, 0), 2)\n",
    "\n",
    "        elif classes_1 < 0.5:\n",
    "            texto = 'Ojo derecho cerrado'\n",
    "            cv2.putText(img, texto, (30,30), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 0, 0), 3)\n",
    "            cv2.putText(img, texto, (30,30), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 255, 0), 2)\n",
    "\n",
    "    else:\n",
    "        print(' no se reconocio los ojos')\n",
    "        \n",
    "cv2.imshow(\"Image\", img)\n",
    "cv2.imwrite(\"dataset/img_test/result.jpg\", img)\n",
    "    \n",
    "#cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
